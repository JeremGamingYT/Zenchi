======================================================================
ğŸ“ DISTILLATION CROSS-ARCHITECTURE
======================================================================
ğŸ”¥ MODE PRODUCTION (OPTIMISÃ‰) - Offline Distillation

ğŸ“Š Configuration:
   num_samples: 200000
   num_epochs: 5
   batch_size: 4
   save_path: ./atlas_distilled_gpt_oss.pt
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘          ğŸ“ DISTILLATION CROSS-ARCHITECTURE ğŸ“               â•‘
â•‘     Transformer (Teacher) â†’ State-Space Model (Student)      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… DonnÃ©es Offline trouvÃ©es: atlas_offline_data.pt
   -> Passage direct Ã  l'entraÃ®nement FAST (Teacher non chargÃ©)
   ğŸ”„ Chargement du Tokenizer seul (pour infÃ©rence future)...

ğŸ“¦ Chargement du PackedDataset (Optimized)...
ğŸ“‚ Chargement des donnÃ©es offline: atlas_offline_data.pt
ğŸ“¦ Packing des sÃ©quences...
âœ… Packing terminÃ©: 200000 samples -> 917 packed sequences
   EfficacitÃ© thÃ©orique: 266.24%
âœ… torch.compile(mode='default') activÃ© pour le student!
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘          ğŸ“ DISTILLATION INTENSIVE GPT-OSS-20B ğŸ“            â•‘
â•‘       Transformer (Teacher) -> State-Space (Student)         â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Epochs: 5  |  Batch: 4  |  Accumulation: 8
â•‘  Effective batch: 32  |  Total steps: 1,145
â•‘  KD: 0.5  |  Hidden: 0.3  |  Task: 0.2
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Epoch 1/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 229/229 [04:12<00:00,  1.10s/it, Step=28, Loss=37.9040, KD=74.2311]  

ğŸ“Š Epoch 1 Summary:
   KD Loss: 268.7012
   Hidden Loss: 0.3049
   Task Loss: 110.7607
   Total Loss: 156.5942
   ğŸ’¾ Saved best model (loss=156.5942)
Epoch 2/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 229/229 [04:09<00:00,  1.09s/it, Step=57, Loss=13.8674, KD=25.8145]

ğŸ“Š Epoch 2 Summary:
   KD Loss: 32.1982
   Hidden Loss: 0.0493
   Task Loss: 8.5587
   Total Loss: 17.8256
   ğŸ’¾ Saved best model (loss=17.8256)
Epoch 3/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 229/229 [04:11<00:00,  1.10s/it, Step=85, Loss=3.3052, KD=6.2917] 

ğŸ“Š Epoch 3 Summary:
   KD Loss: 11.5805
   Hidden Loss: 0.0159
   Task Loss: 5.2153
   Total Loss: 6.8381
   ğŸ’¾ Saved best model (loss=6.8381)
Epoch 4/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 229/229 [04:10<00:00,  1.09s/it, Step=114, Loss=1.2062, KD=1.1783]

ğŸ“Š Epoch 4 Summary:
   KD Loss: 2.6416
   Hidden Loss: 0.0088
   Task Loss: 2.7185
   Total Loss: 1.8671
   ğŸ’¾ Saved best model (loss=1.8671)
Epoch 5/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 229/229 [04:11<00:00,  1.10s/it, Step=143, Loss=1.0425, KD=1.5719]

ğŸ“Š Epoch 5 Summary:
   KD Loss: 1.5752
   Hidden Loss: 0.0071
   Task Loss: 1.6677
   Total Loss: 1.1233
   ğŸ’¾ Saved best model (loss=1.1233)

âœ… Distillation terminÃ©e!
   Best loss: 1.1233
   Model saved to: ./atlas_distilled_gpt_oss.pt
   âœ… Utilisation du tokenizer HuggingFace!

======================================================================
âœ… DISTILLATION TERMINÃ‰E!
   ModÃ¨le sauvegardÃ©: ./atlas_distilled_gpt_oss.pt
======================================================================

ğŸ§¹ Nettoyage mÃ©moire GPU...
   âœ“ Pipeline supprimÃ©
   âœ“ Distiller supprimÃ©
   âœ“ Cache CUDA vidÃ© - 81.49 GB libres
âœ… MÃ©moire GPU nettoyÃ©e!

ğŸ’‰ Injection de connaissances...
âœ… InjectÃ© 2 rÃ¨gles logiques

ğŸš€ DÃ©marrage de l'entraÃ®nement...
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              ğŸš€ DÃ‰MARRAGE ENTRAÃNEMENT ATLAS ğŸš€              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•