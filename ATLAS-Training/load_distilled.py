import torch
import sys
import os
from transformers import AutoTokenizer

# Assure-toi que atlas_core.py est dans le path ou dans le dossier courant
# Note: L'import de 'atlas_core' va lancer l'installation des d√©pendances d√©finie au d√©but du fichier.
try:
    from atlas_core import ATLAS, ATLASConfig, ATLASInference, DemoTokenizer, DEVICE
except ImportError:
    print("‚ö†Ô∏è Impossible d'importer 'atlas_core.py'. Assurez-vous qu'il est dans le m√™me dossier ou le PYTHONPATH.")
    # On peut essayer d'ajouter le dossier courant
    sys.path.append(os.getcwd())
    try:
        from atlas_core import ATLAS, ATLASConfig, ATLASInference, DemoTokenizer, DEVICE
    except ImportError as e:
        print(f"‚ùå Erreur critique d'import: {e}")
        raise

def load_distilled_model(
    checkpoint_path: str,
    device: str = None,
    use_teacher_tokenizer: bool = True,
    teacher_name: str = "mistralai/Mistral-7B-Instruct-v0.2"
):
    """
    Charge un mod√®le ATLAS distill√© compatible Notebook.
    
    Args:
        checkpoint_path: Chemin vers le fichier .pt (ex: 'atlas_distilled_gpt_oss.pt')
        device: 'cuda' ou 'cpu' (d√©faut: auto d√©tect√© par atlas_core.py)
        use_teacher_tokenizer: Si True, charge un tokenizer HuggingFace. Sinon DemoTokenizer.
        teacher_name: Nom du mod√®le HF pour le tokenizer (doit correspondre √† celui utilis√© lors de la distillation)
    
    Returns:
        inference: Une instance ATLASInference pr√™te √† l'emploi
    """
    
    current_device = device if device else DEVICE
    print(f"üîÑ Chargement du mod√®le depuis {checkpoint_path} sur {current_device}...")

    # 1. Reconstruire la configuration
    # IMPORTANT: Doit correspondre √† la config utilis√©e dans main() de atlas_core.py lors de l'entra√Ænement
    print("üìã Configuration du mod√®le...")
    config = ATLASConfig(
        # Dimensions utilis√©es dans main()
        d_model=1024,
        n_layers=24,
        d_state=128,
        
        # Vocabulary
        vocab_size=50257,
        max_seq_len=4096,
        
        # Autres param√®tres par d√©faut de main()
        certainty_threshold=0.85,
        verification_passes=3
    )
    
    # 2. Cr√©er l'instance du mod√®le
    print("üîß Instanciation de l'architecture ATLAS...")
    model = ATLAS(config)
    
    # 3. Charger les poids
    if os.path.exists(checkpoint_path):
        try:
            checkpoint = torch.load(checkpoint_path, map_location=current_device)
            
            # Gestion des diff√©rentes structures de sauvegarde
            if 'model_state_dict' in checkpoint:
                state_dict = checkpoint['model_state_dict']
            else:
                state_dict = checkpoint # Supposons que c'est le state_dict direct
            
            # Chargement strict=False pour √©viter les erreurs si des buffers auxiliaires manquent
            keys = model.load_state_dict(state_dict, strict=False)
            print(f"‚úÖ Poids charg√©s! (Missing: {len(keys.missing_keys)}, Unexpected: {len(keys.unexpected_keys)})")
            
        except Exception as e:
            print(f"‚ùå Erreur lors du chargement des poids: {e}")
            return None
    else:
        print(f"‚ö†Ô∏è Fichier checkpoint introuvable: {checkpoint_path}")
        return None

    model.to(current_device)
    model.eval()

    # 4. Charger le Tokenizer
    print("üî§ Configuration du tokenizer...")
    if use_teacher_tokenizer:
        try:
            print(f"   Tentative de chargement du tokenizer HF: {teacher_name}")
            tokenizer = AutoTokenizer.from_pretrained(teacher_name, trust_remote_code=True)
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            print("   ‚úÖ Tokenizer HF charg√©.")
        except Exception as e:
            print(f"   ‚ö†Ô∏è Echec chargement tokenizer HF ({e}). Fallback sur DemoTokenizer.")
            tokenizer = DemoTokenizer(vocab_size=config.vocab_size)
    else:
        tokenizer = DemoTokenizer(vocab_size=config.vocab_size)

    # 5. Cr√©er l'interface d'inf√©rence
    inference = ATLASInference(model, tokenizer)
    print("\nüöÄ Mod√®le pr√™t √† l'emploi!")
    return inference

# ==========================================
# Exemple d'utilisation pour Notebook
# ==========================================
if __name__ == "__main__":
    # Exemple: Chargeons le mod√®le si le fichier existe
    distilled_path = "atlas_distilled_gpt_oss.pt"
    
    if os.path.exists(distilled_path):
        atlas = load_distilled_model(distilled_path)
        
        if atlas:
            # Test rapide
            response = atlas.answer("Pourquoi le ciel est bleu?", mode="causal")
            print(f"\nüí¨ R√©ponse:\n{response['response']}")
    else:
        print(f"Pour tester, lancez l'entra√Ænement dans atlas_core.py d'abord pour cr√©er {distilled_path}")
